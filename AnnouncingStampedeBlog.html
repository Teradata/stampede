<!DOCTYPE html>  
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>AnnouncingStampedeBlog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
/* 
   This document has been created with Marked.app <http://markedapp.com>, Copyright 2011 Brett Terpstra
   Please leave this notice in place, along with any additional credits below.
   ---------------------------------------------------------------
   Title: GitHub
   Author: Brett Terpstra
   Description: Github README style. Includes theme for Pygmentized code blocks.
*/
html,body{color:black}*{margin:0;padding:0}body{font:13.34px helvetica,arial,freesans,clean,sans-serif;-webkit-font-smoothing:antialiased;line-height:1.4;padding:3px;background:#fff;border-radius:3px;-moz-border-radius:3px;-webkit-border-radius:3px}p{margin:1em 0}a{color:#4183c4;text-decoration:none}#wrapper{background-color:#fff;border:3px solid #eee!important;padding:0 30px;margin:15px}#wrapper{font-size:14px;line-height:1.6}#wrapper>*:first-child{margin-top:0!important}#wrapper>*:last-child{margin-bottom:0!important}h1,h2,h3,h4,h5,h6{margin:0;padding:0}h1{margin:15px 0;padding-bottom:2px;font-size:24px;border-bottom:1px solid #eee}h2{margin:20px 0 10px 0;font-size:18px}h3{margin:20px 0 10px 0;padding-bottom:2px;font-size:14px;border-bottom:1px solid #ddd}h4{font-size:14px;line-height:26px;padding:18px 0 4px;font-weight:bold;text-transform:uppercase}h5{font-size:13px;line-height:26px;padding:14px 0 0;font-weight:bold;text-transform:uppercase}h6{color:#666;font-size:14px;line-height:26px;padding:18px 0 0;font-weight:normal;font-variant:italic}hr{background:transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;border:0 none;color:#ccc;height:4px;margin:20px 0;padding:0}#wrapper>h2:first-child,#wrapper>h1:first-child,#wrapper>h1:first-child+h2{border:0;margin:0;padding:0}#wrapper>h3:first-child,#wrapper>h4:first-child,#wrapper>h5:first-child,#wrapper>h6:first-child{margin:0;padding:0}h4+p,h5+p,h6+p{margin-top:0}li p.first{display:inline-block}ul,ol{margin:15px 0 15px 25px}ul li,ol li{margin-top:7px;margin-bottom:7px}ul li>*:last-child,ol li>*:last-child{margin-bottom:0}ul li>*:first-child,ol li>*:first-child{margin-top:0}#wrapper>ul,#wrapper>ol{margin-top:21px;margin-left:36px}dl{margin:0;padding:20px 0 0}dl dt{font-size:14px;font-weight:bold;line-height:normal;margin:0;padding:20px 0 0}dl dt:first-child{padding:0}dl dd{font-size:13px;margin:0;padding:3px 0 0}blockquote{margin:14px 0;border-left:4px solid #ddd;padding-left:11px;color:#555}table{border-collapse:collapse;margin:20px 0 0;padding:0}table tr{border-top:1px solid #ccc;background-color:#fff;margin:0;padding:0}table tr:nth-child(2n){background-color:#f8f8f8}table tr th,table tr td{border:1px solid #ccc;text-align:left;margin:0;padding:6px 13px}img{max-width:100%;height:auto}code,tt{margin:0 2px;padding:2px 5px;white-space:nowrap;border:1px solid #ccc;background-color:#f8f8f8;border-radius:3px;-moz-border-radius:3px;-webkit-border-radius:3px;font-size:12px}pre>code{margin:0;padding:0;white-space:pre;border:0;background:transparent;font-size:13px}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px;-moz-border-radius:3px;-webkit-border-radius:3px}#wrapper>pre,#wrapper>div.highlight{margin:10px 0 0}pre code,pre tt{background-color:transparent;border:0}#wrapper{background-color:#fff;border:1px solid #cacaca;padding:30px}.poetry pre{font-family:Georgia,Garamond,serif!important;font-style:italic;font-size:110%!important;line-height:1.6em;display:block;margin-left:1em}.poetry pre code{font-family:Georgia,Garamond,serif!important}sup,sub,a.footnote{font-size:1.4ex;height:0;line-height:1;vertical-align:super;position:relative}sub{vertical-align:sub;top:-1px}@media print{body{background:#fff}img,pre,blockquote,table,figure{page-break-inside:avoid}#wrapper{background:#fff;border:0}code{background-color:#fff;color:#444!important;padding:0 .2em;border:1px solid #dedede}pre code{background-color:#fff!important;overflow:visible}pre{background:#fff}}@media screen{body.inverted,.inverted #wrapper,.inverted hr .inverted p,.inverted td,.inverted li,.inverted h1,.inverted h2,.inverted h3,.inverted h4,.inverted h5,.inverted h6,.inverted th,.inverted .math,.inverted caption,.inverted dd,.inverted dt,.inverted blockquote{color:#eee!important;border-color:#555}.inverted td,.inverted th{background:#333}.inverted pre,.inverted code,.inverted tt{background:#444!important}.inverted h2{border-color:#555}.inverted hr{border-color:#777;border-width:1px!important}::selection{background:rgba(157,193,200,.5)}h1::selection{background-color:rgba(45,156,208,.3)}h2::selection{background-color:rgba(90,182,224,.3)}h3::selection,h4::selection,h5::selection,h6::selection,li::selection,ol::selection{background-color:rgba(133,201,232,.3)}code::selection{background-color:rgba(0,0,0,.7);color:#eee}code span::selection{background-color:rgba(0,0,0,.7)!important;color:#eee!important}a::selection{background-color:rgba(255,230,102,.2)}.inverted a::selection{background-color:rgba(255,230,102,.6)}td::selection,th::selection,caption::selection{background-color:rgba(180,237,95,.5)}.inverted{background:#0b2531}.inverted #wrapper,.inverted{background:rgba(37,42,42,1)}.inverted a{color:rgba(172,209,213,1)}}.highlight .c{color:#998;font-style:italic}.highlight .err{color:#a61717;background-color:#e3d2d2}.highlight .k{font-weight:bold}.highlight .o{font-weight:bold}.highlight .cm{color:#998;font-style:italic}.highlight .cp{color:#999;font-weight:bold}.highlight .c1{color:#998;font-style:italic}.highlight .cs{color:#999;font-weight:bold;font-style:italic}.highlight .gd{color:#000;background-color:#fdd}.highlight .gd .x{color:#000;background-color:#faa}.highlight .ge{font-style:italic}.highlight .gr{color:#a00}.highlight .gh{color:#999}.highlight .gi{color:#000;background-color:#dfd}.highlight .gi .x{color:#000;background-color:#afa}.highlight .go{color:#888}.highlight .gp{color:#555}.highlight .gs{font-weight:bold}.highlight .gu{color:#800080;font-weight:bold}.highlight .gt{color:#a00}.highlight .kc{font-weight:bold}.highlight .kd{font-weight:bold}.highlight .kn{font-weight:bold}.highlight .kp{font-weight:bold}.highlight .kr{font-weight:bold}.highlight .kt{color:#458;font-weight:bold}.highlight .m{color:#099}.highlight .s{color:#d14}.highlight .na{color:#008080}.highlight .nb{color:#0086b3}.highlight .nc{color:#458;font-weight:bold}.highlight .no{color:#008080}.highlight .ni{color:#800080}.highlight .ne{color:#900;font-weight:bold}.highlight .nf{color:#900;font-weight:bold}.highlight .nn{color:#555}.highlight .nt{color:#000080}.highlight .nv{color:#008080}.highlight .ow{font-weight:bold}.highlight .w{color:#bbb}.highlight .mf{color:#099}.highlight .mh{color:#099}.highlight .mi{color:#099}.highlight .mo{color:#099}.highlight .sb{color:#d14}.highlight .sc{color:#d14}.highlight .sd{color:#d14}.highlight .s2{color:#d14}.highlight .se{color:#d14}.highlight .sh{color:#d14}.highlight .si{color:#d14}.highlight .sx{color:#d14}.highlight .sr{color:#009926}.highlight .s1{color:#d14}.highlight .ss{color:#990073}.highlight .bp{color:#999}.highlight .vc{color:#008080}.highlight .vg{color:#008080}.highlight .vi{color:#008080}.highlight .il{color:#099}.highlight .gc{color:#999;background-color:#eaf2f5}.type-csharp .highlight .k{color:#00F}.type-csharp .highlight .kt{color:#00F}.type-csharp .highlight .nf{color:#000;font-weight:normal}.type-csharp .highlight .nc{color:#2b91af}.type-csharp .highlight .nn{color:#000}.type-csharp .highlight .s{color:#a31515}.type-csharp .highlight .sc{color:#a31515}
</style>

</head>
<body class="normal">
  <div id="wrapper">
      <h1 id="announcingstampede">Announcing &#8220;Stampede&#8221;</h1>

<p>Dean Wampler<br/>
<a href="&#109;&#x61;&#105;&#x6c;&#116;&#111;&#x3a;&#100;&#x65;&#x61;&#110;&#46;&#x77;&#97;&#109;&#x70;&#108;&#101;&#114;&#x40;&#116;&#x68;&#x69;&#110;&#x6b;&#98;&#x69;&#103;&#97;&#110;&#x61;&#x6c;&#x79;&#x74;&#x69;&#x63;&#115;&#46;&#x63;&#x6f;&#109;">&#x64;&#101;&#x61;&#x6e;&#x2e;&#x77;&#x61;&#x6d;&#x70;&#x6c;&#101;&#114;&#64;&#116;&#104;&#x69;&#x6e;&#x6b;&#x62;&#x69;&#103;&#x61;&#x6e;&#x61;&#x6c;&#121;&#x74;&#x69;&#x63;&#x73;&#x2e;&#x63;&#111;&#x6d;</a><br/>
<a href="https://twitter.com/thinkBigA/">@thinkBigA</a><br/>
January 9, 2013</p>

<p>When you&#8217;re building nontrivial workflows, you need a tool that lets you express the dependencies between tasks, schedule their execution, detect failures and attempt retries, etc. You also want that tool to be concise, easy to use, yet powerful.</p>

<p>Welcome to <em>Stampede</em>, the workflow tool that works as <a href="http://en.wikipedia.org/wiki/Cthulhu">Cthulhu</a> intended for *nix systems, using <code>make</code> for dependency management and task seqeuencing, <code>bash</code> for scripting, and <code>cron</code> for scheduling.</p>

<p><em>Stampede</em> originated as an alternative workflow tool for <a href="http://hadoop.apache.org">Hadoop</a>, but it is not limited to Hadoop scenarios.</p>

<h2 id="embracingtheunixphilosophy">Embracing the Unix Philosophy</h2>

<p><em>Stampede</em> was born out of frustration with heavyweight &#8220;enterprisey&#8221; tools that are hard and frustrating to use. We have a ~40-year tradition, <em>the Unix Philosophy</em>, of flyweight, flexible tools that compose together to build sophisticated applications.</p>

<p>How can you specify dependencies between tasks? <code>Make</code> does this concisely and flexibly. How do you script the tasks themselves? One of the powerful Unix shells, such as <code>bash</code>, is platform portable and supports the concise expression of complex tasks. How do you schedule when a workflow should start? <code>Cron</code> and its sibling <code>at</code> make this easy.</p>

<p><em>Stampede</em> won&#8217;t appeal to you unless you know <code>make</code> and <code>bash</code>. It doesn&#8217;t provide a GUI (at least not yet). It&#8217;s a tool for <a href="http://polyglotprogramming.com">polygot programmers</a>, developers who use a diverse set of languages and tools, adopting the most appropriate tool for a given job. If the word <a href="http://devops.com/">DevOps</a> means anything to you, then <em>Stampede</em> is the tool for you.</p>

<h2 id="howdoesitwork">How Does It Work?</h2>

<p>In fact, <em>Stampede</em> is less than meets the eye. Really. Most of its power comes from <code>make</code>, <code>bash</code>, and other *nix command-line tools, like <code>date</code>, <code>mkdir</code>, and their friends. However, those tools by themselves aren&#8217;t quite enough for convenient development of workflows, which we call <em>stampedes</em>.</p>

<p>So, <em>Stampede</em> adds lots of helper tools, mostly <code>bash</code> scripts, to make it easier to do common IT tasks, like specify yesterday&#8217;s date for an ETL process, watch for a file to appear in a drop zone from an FTP process and then start processing it, retry a failed workflow every hour until it succeeds, etc. <em>Stampede</em> also includes a driver script, called <code>stampede</code> that does various environment setup steps before calling <code>make</code>. Your actual workflows (<em>stampedes</em>) are defined in <code>Makefiles</code>.</p>

<p>In principle, <em>Stampede</em> can support any *nix environment, but currently we only support Linux and Mac OSX. So, we require <code>bash</code> for scripts and <a href="http://www.gnu.org/software/make/">Gnu Make</a>, since these are the standard tools distributed with Linux, Mac OSX, and also Cygwin. Cygwin support should be possible and we welcome patches if anyone wants to take it on. Any Unix system with
<code>bash</code> and Gnu <code>make</code> installed should also be able to run <em>Stampede</em> out of the box. Patches are welcome if you encounter problems.</p>

<p>Here is an example <code>Makefile</code> for a fictitious Hadoop workflow, taken from the distribution&#8217;s Hadoop example. We&#8217;ll use the environment variable <code>$STAMPEDE_HOME</code> to reference where you installed <em>Stampede</em>. (It&#8217;s value is set by the <code>stampede</code> driver script when you run a workflow.) The <code>Makefile</code> comments describe what&#8217;s going on:</p>

<pre><code># Example Makefile for a Stampede project for a Hadoop workflow.
# For more details, see $STAMPEDE_HOME/examples/hadoop/README.md.

# Call the &quot;ymd&quot; and &quot;yesterday-ymd&quot; tools (bash scripts that 
# are part of Stampede) to get the YYYY-MM-DD for today and 
# yesterday, respectively, e.g., 2013-01-01 and 2012-12-31:
YMD           = $(shell ymd '-')
YESTERDAY_YMD = $(shell yesterday-ymd '-')

# Local (as opposed to HDFS) file system location where FTP'ed incoming
# files are dropped. 
DROP_ZONE = /var/ftp/drop-zone

# Locations in HDFS for the ingested files for yesterday.
HDFS_FTP_YYMD_DIR = /ftp/${YESTERDAY_YMD}
HDFS_ORDERS       = /orders/${YESTERDAY_YMD}

# Data from our &quot;partners&quot;, BargainMonsters.com and ElectronicsHut.com
BM_FILE      = bargain-monster-orders-${YESTERDAY_YMD}.gzip
EH_FILE      = electronics-hut-orders-${YESTERDAY_YMD}.gzip
BM_FTP_FILE  = ${DROP_ZONE}/${BM_FILE}
EH_FTP_FILE  = ${DROP_ZONE}/${EH_FILE}

# Data used by our recommendation engine that analyzes click streams and orders.
RECOMMENDER_DATA_DIR = /recommendation-engine/clicks-orders

# The location for Hive's internal/managed tables, given by the property:
#   hive.metastore.warehouse.dir
HIVE_WAREHOUSE_DIR = $(shell hive-prop --print-value hive.metastore.warehouse.dir)

# URL for the NameNode.
HADOOP_NAMENODE = $(shell mapreduce-prop --print-value )

HADOOP = hadoop
PIG    = pig
HIVE   = hive
SQOOP  = sqoop

all: etl analysis export
  @echo Hadoop stampede finished!

etl: ingest cleanse

ingest: from-production-db from-ftp-drop-zone

# Use Sqoop to ingest yesterday's click stream data from the production database.
from-production-db:
  @echo &quot;Ingesting clickstream data for yesterday: ${YESTERDAY_YMD} (today: ${YMD})
  ${SQOOP} import \
    --connect jdbc:mysql://db-server:3306/clickstream-prod \
    --username some_user -P \
    --table adclicks \
    --query &quot;select * from adclicks where ymd = '${YESTERDAY_YMD}';&quot; \
    --num-mappers 5 \
    --hive-import

from-ftp-drop-zone: ${BM_FTP_FILE} ${EH_FTP_FILE}

# Wait up to 4 hours, checking every 10 minutes, for yesterday's data from 
# BargainMonster.com and ElectronicsHut.com of orders that originated
# as ad clicks. Once each arrives, put it in HDFS.
${BM_FTP_FILE} ${EH_FTP_FILE}: ${HDFS_FTP_YYMD_DIR}
  @try-for 4h 10m 'test -f $@'
  ${HADOOP} fs -put $@ ${HDFS_FTP_YYMD_DIR} 

${HDFS_FTP_YYMD_DIR}:
  ${HADOOP} fs -mkdir ${HDFS_FTP_YYMD_DIR}

# Use Pig for data cleansing. Pass in parameters that tell the &quot;cleanse-orders.pig&quot;
# script the location of the input and where to write the output (both in HDFS).
cleanse:
  ${PIG} \
    -param INPUT_DIR=${HDFS_FTP_YYMD_DIR} \
    -param OUTPUT_DIR=${HDFS_ORDERS} \
    -f cleanse-orders.pig 

analysis: reports-analysis recommendations-analysis

# Treat the output directory of the Pig script, &quot;${HDFS_ORDERS}&quot; as the
# location of a partition for a Hive external &quot;orders&quot; table. The Hive script
# &quot;clicks-orders-report.hql&quot; will use ALTER TABLE to add this partition, so
# we pass in the location as an $ORDERS_DIR defined variable. The other 
# variable we'll define is &quot;YMD&quot; which will be used for processing; we set it 
# to yesterday's date. The script will also use the internal &quot;adclicks&quot; table 
# created by the previous Sqoop task in the workflow.
reports-analysis:
  ${HIVE} \
    --define ORDERS_DIR=${HDFS_ORDERS} \
    --define YMD=${YESTERDAY_YMD} \
    -f clicks-orders-report.hql 

# A custom Hadoop job that updates the data for a recommendation engine. 
# We assume the Hive clicks data is in the Hive &quot;warehouse&quot; location, inside
# a &quot;finance&quot; database (in a subdirectory named &quot;finance.db&quot;), and an
# &quot;adclicks&quot; subdirectory for the table data.
recommendations-analysis:
  ${HADOOP} \
    jar /usr/local/mycompany/clicks-orders-recommendations.jar \
    --clicks=${HIVE_WAREHOUSE_DIR}/finance.db/adclicks \
    --orders=${HDFS_ORDERS} \
    --ymd=${YESTERDAY_YMD} \
    --output=${RECOMMENDER_DATA_DIR}

# Using Sqoop, export the results of both analysis steps back to tables in
# another database.
export: reports-analysis-export recommendations-analysis-export

reports-analysis-export:
  ${SQOOP} export \
    --connect jdbc:mysql://db-server:3306/orders-warehouse \
    --username uname -P
    --table clicks_orders \
    --num-mappers 5 \
    --export-dir ${HIVE_WAREHOUSE_DIR}/finance.db/clicks_orders_analysis

recommendations-analysis-export:
  ${SQOOP} export \
    --connect jdbc:mysql://db-server:3306/recommendations-prod \
    --username uname -P
    --table clicks_orders_recommendations \
    --num-mappers 5 \
    --export-dir ${RECOMMENDER_DATA_DIR}
</code></pre>

<h2 id="hadoopsupport">Hadoop Support</h2>

<p><em>Stampede</em> originated as a tool for Hadoop-related projects, although it&#8217;s not limited to those scenarios.</p>

<p>As you can see from the previous example, because Hadoop tools have command-line interfaces, we simply call them in the <code>Makefile</code>.</p>

<p>The additional Hadoop support consists of <code>bash</code> scripts and compiled Java code in the <code>$STAMPEDE_HOME/bin/hadoop</code> directory. </p>

<p>Currently, there are three additional tools provided by <em>Stampede</em> for determining configuration property settings for <em>MapReduce</em>, <em>Hive</em>, and <em>Pig</em>, by actually running those tools, as opposed to reading static configuration files. More Hadoop-specific tools are planned, e.g., basic integration with the <em>JobTracker</em>, <em>NameNode</em>, and <em>HCatalog</em>.</p>

<h2 id="wheretogofromhere">Where to Go from Here</h2>

<p>Download a release or clone the <a href="https://github.com/ThinkBigAnalytics/stampede">Stampede GitHub repo</a> and follow the instuctions in the <a href="https://github.com/ThinkBigAnalytics/stampede">README</a> for installing <em>Stampede</em> and using it. You&#8217;ll also find the Hadoop example we discussed above in <code>$STAMPEDE_HOME/examples/hadoop</code>. See also our <a href="https://github.com/ThinkBigAnalytics/stampede/wiki">GitHub Wiki</a>.</p>

<p>We hope you find <em>Stampede</em> useful. Consider joining our Google Group, <a href="https://groups.google.com/forum/#!forum/stampede-users">stampede-users</a>.</p>
    </div>
</body>
</html>